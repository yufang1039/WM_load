{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d1acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique 4-character words (chengyu + four_char_words): 159\n"
     ]
    }
   ],
   "source": [
    "# Read chengyu list from txt file, keep only 4-character entries\n",
    "chengyu_txt_path = \"/Users/yufang/WM_load/word_database_txt/four_3_syllable_words.txt\"\n",
    "with open(chengyu_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chengyu_list = [line.strip() for line in f if len(line.strip()) == 3]\n",
    "\n",
    "# Combine chengyu_list and four_char_words without duplication\n",
    "combined_list = chengyu_list\n",
    "\n",
    "print(f\"Total number of unique 4-character words (chengyu + four_char_words): {len(combined_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f38e69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cue type counts:\n",
      "(0, 1): 82\n",
      "(0, 2): 41\n",
      "(1, 0): 41\n",
      "(1, 2): 41\n",
      "(2, 0): 41\n",
      "(2, 1): 41\n",
      "(3, 0): 41\n",
      "(3, 1): 41\n",
      "(3, 2): 41\n",
      "Corrected cue type counts:\n",
      "(0, 1): 41\n",
      "(0, 2): 41\n",
      "(1, 0): 41\n",
      "(1, 2): 41\n",
      "(2, 0): 41\n",
      "(2, 1): 41\n",
      "(3, 0): 41\n",
      "(3, 1): 41\n",
      "(3, 2): 41\n",
      "Saved 369 trials\n",
      "First few trials:\n",
      "    W1   W2   W3   W4  Cue_Word  Cue_Pos Cue\n",
      "0  重要性  运输车  科学馆  销售额         1        1   要\n",
      "1  保险箱  高利贷  纪念碑  参谋掌         1        1   险\n",
      "2  营业员  太阳能  神经元  责任感         1        1   业\n",
      "3  抗生素  保险箱  黑板报  营业员         1        1   生\n",
      "4  锦标赛  主人翁  自来水  科学家         1        1   标\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------\n",
    "# HYPERPARAMETERS\n",
    "# ----------------------\n",
    "N_positions = 3                # Number of possible cue positions in a word (e.g., 4 for 4-char words)\n",
    "N_trials_per_position = 41     # Number of trials for each cue position type\n",
    "N_words_each_trial = 4         # Number of words in each trial (e.g., 3)\n",
    "N_words = len(chengyu_list)    # Total number of available words\n",
    "\n",
    "words_list = chengyu_list.copy()\n",
    "random.shuffle(words_list)\n",
    "\n",
    "# ----------------------\n",
    "# COLUMN NAMES\n",
    "# ----------------------\n",
    "word_columns = [f\"W{i+1}\" for i in range(N_words_each_trial)]\n",
    "\n",
    "# ----------------------\n",
    "# CONSTRAINTS\n",
    "# ----------------------\n",
    "# - No trial should have duplicated words\n",
    "# - Each (word, cue position) combination can appear at most once\n",
    "# - Don't cue position where the index of character and index of word is the same (i.e., don't cue 1st char in 1st word, 2nd char in 2nd word, etc.)\n",
    "\n",
    "# ----------------------\n",
    "# TRACKERS\n",
    "# ----------------------\n",
    "used_word_cuepos = set()  # (word, cue_pos) pairs used\n",
    "used_trials = set()       # frozenset of words in a trial, to avoid duplicate trials\n",
    "\n",
    "trials = []\n",
    "\n",
    "# ----------------------\n",
    "# STEP 1: Generate trials for each cue position type\n",
    "# ----------------------\n",
    "# The original code creates 40 trials for (0,1) and then for every (word_idx, char_idx) where word_idx != char_idx,\n",
    "# it creates 40 more for each, including (1,1), (2,2), (3,3) -- but those are excluded by the if word_idx != char_idx.\n",
    "# So, let's print out the cue_types to see what's happening.\n",
    "\n",
    "cue_types = []\n",
    "# First N_trials_per_position: cue second char of first word\n",
    "cue_types.extend([(0, 1)] * N_trials_per_position)\n",
    "\n",
    "# For the rest, enumerate all valid (word_idx, char_idx) pairs, except where word_idx == char_idx\n",
    "for word_idx in range(N_words_each_trial):\n",
    "    for char_idx in range(N_positions):\n",
    "        if word_idx != char_idx:\n",
    "            cue_types.extend([(word_idx, char_idx)] * N_trials_per_position)\n",
    "\n",
    "# Let's check how many times (0,1) appears in cue_types\n",
    "from collections import Counter\n",
    "cue_type_counts = Counter(cue_types)\n",
    "print(\"Cue type counts:\")\n",
    "for k, v in sorted(cue_type_counts.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# The reason (0,1) appears 80 times is:\n",
    "# - First, you explicitly add 40 of (0,1)\n",
    "# - Then, in the nested loop, for word_idx=0, char_idx=1, you add another 40 of (0,1)\n",
    "#   (since word_idx != char_idx, so (0,1) is valid and added again)\n",
    "# So, (0,1) is added twice, for a total of 80.\n",
    "\n",
    "# To fix this, you should not add (0,1) in both places.\n",
    "# Solution: Only add (0,1) in the nested loop, and remove the explicit first 40.\n",
    "# Or, if you want the first 40 to be \"special\", then in the nested loop, skip (0,1).\n",
    "\n",
    "# Here's the corrected code:\n",
    "\n",
    "cue_types = []\n",
    "# First N_trials_per_position: cue second char of first word\n",
    "cue_types.extend([(0, 1)] * N_trials_per_position)\n",
    "\n",
    "# For the rest, enumerate all valid (word_idx, char_idx) pairs, except where word_idx != char_idx and (word_idx, char_idx) != (0,1)\n",
    "for word_idx in range(N_words_each_trial):\n",
    "    for char_idx in range(N_positions):\n",
    "        if word_idx != char_idx and not (word_idx == 0 and char_idx == 1):\n",
    "            cue_types.extend([(word_idx, char_idx)] * N_trials_per_position)\n",
    "\n",
    "# Now, (0,1) will only appear 40 times.\n",
    "\n",
    "# Let's check again\n",
    "cue_type_counts = Counter(cue_types)\n",
    "print(\"Corrected cue type counts:\")\n",
    "for k, v in sorted(cue_type_counts.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Total number of trials to generate\n",
    "N_total_trials = len(cue_types)\n",
    "\n",
    "# ----------------------\n",
    "# STEP 2: Generate trials\n",
    "# ----------------------\n",
    "# To ensure we can always find enough valid trials, we may need to try multiple times\n",
    "attempts = 0\n",
    "max_attempts = N_total_trials * 100\n",
    "\n",
    "while len(trials) < N_total_trials and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    # Randomly sample N_words_each_trial distinct words\n",
    "    trial_words = tuple(random.sample(words_list, N_words_each_trial))\n",
    "    if len(set(trial_words)) < N_words_each_trial:\n",
    "        continue  # skip if not all words are unique\n",
    "\n",
    "    trial_set = frozenset(trial_words)\n",
    "    if trial_set in used_trials:\n",
    "        continue  # skip duplicate trial\n",
    "\n",
    "    # Get the cue type for this trial\n",
    "    trial_idx = len(trials)\n",
    "    word_idx, char_idx = cue_types[trial_idx]\n",
    "\n",
    "    word_for_cue = trial_words[word_idx]\n",
    "    # Check if this (word, cue_pos) has been used\n",
    "    if (word_for_cue, char_idx) in used_word_cuepos:\n",
    "        continue\n",
    "\n",
    "    # All constraints satisfied, add trial\n",
    "    used_trials.add(trial_set)\n",
    "    used_word_cuepos.add((word_for_cue, char_idx))\n",
    "    cue_char = word_for_cue[char_idx]\n",
    "    trial_dict = {col: w for col, w in zip(word_columns, trial_words)}\n",
    "    trial_dict['Cue'] = cue_char\n",
    "    trial_dict['Cue_Word'] = word_idx + 1  # 1-based index\n",
    "    trial_dict['Cue_Pos'] = char_idx\n",
    "    trials.append(trial_dict)\n",
    "\n",
    "if len(trials) < N_total_trials:\n",
    "    raise RuntimeError(f\"Could not generate enough valid trials ({len(trials)}/{N_total_trials}) with the given constraints.\")\n",
    "\n",
    "# ----------------------\n",
    "# STEP 3: Save to DataFrame\n",
    "# ----------------------\n",
    "results = pd.DataFrame(trials)\n",
    "# Reorder columns for clarity\n",
    "cols = word_columns + ['Cue_Word', 'Cue_Pos', 'Cue']\n",
    "results = results[cols]\n",
    "\n",
    "results.to_csv(\"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_3_syllable_trials.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved {len(results)} trials\")\n",
    "print(\"First few trials:\")\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d26de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized and added 'block' column to /Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_3_syllable_trials_with_blocks.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "input_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_4_syllable_exp.csv\"\n",
    "output_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_4_syllable_exp_with_blocks.csv\"\n",
    "input_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp.csv\"\n",
    "output_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp_with_blocks.csv\"\n",
    "input_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_3_syllable_exp.csv\"\n",
    "output_csv = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_3_syllable_exp_with_blocks.csv\"\n",
    "\n",
    "# Number of trials per block\n",
    "N = 41\n",
    "\n",
    "# Read all rows from the input CSV\n",
    "with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "    reader = list(csv.reader(infile))\n",
    "    header = reader[0]\n",
    "    data_rows = reader[1:]\n",
    "\n",
    "# Shuffle the data rows randomly\n",
    "random.shuffle(data_rows)\n",
    "\n",
    "# Assign block numbers\n",
    "for idx, row in enumerate(data_rows):\n",
    "    block_num = idx // N + 1\n",
    "    row.append(str(block_num))\n",
    "\n",
    "# Write to the new CSV with the 'block' column\n",
    "with open(output_csv, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Write header with new 'block' column\n",
    "    writer.writerow(header + ['block'])\n",
    "    # Write data rows with block numbers\n",
    "    writer.writerows(data_rows)\n",
    "\n",
    "print(f\"Randomized and added 'block' column to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5c192",
   "metadata": {},
   "source": [
    "# Three Syllable words processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5095a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 146 distinct words.\n",
      "Words saved to: /Users/yufang/WM_load/word_database_txt/three_4_syllable_words.txt\n",
      "\n",
      "Distinct words:\n",
      "一举两得\n",
      "一往无前\n",
      "一成不变\n",
      "一诺千金\n",
      "一败涂地\n",
      "七步成诗\n",
      "万无一失\n",
      "万紫千红\n",
      "三顾茅庐\n",
      "东施效颦\n",
      "举一反三\n",
      "举案齐眉\n",
      "乐善好施\n",
      "五湖四海\n",
      "仁至义尽\n",
      "信守不渝\n",
      "光明正大\n",
      "光明磊落\n",
      "凿壁偷光\n",
      "刮目相看\n",
      "刻舟求剑\n",
      "前功尽弃\n",
      "力争上游\n",
      "功亏一篑\n",
      "功败垂成\n",
      "势不可挡\n",
      "勇往直前\n",
      "十拿九稳\n",
      "千军万马\n",
      "千载难逢\n",
      "半途而废\n",
      "博古通今\n",
      "卧薪尝胆\n",
      "叶公好龙\n",
      "同甘共苦\n",
      "后生可畏\n",
      "唇亡齿寒\n",
      "唇枪舌剑\n",
      "喜出望外\n",
      "囊萤映雪\n",
      "四面楚歌\n",
      "囫囵吞枣\n",
      "围魏救赵\n",
      "国泰民安\n",
      "声势浩大\n",
      "夜以继日\n",
      "大公无私\n",
      "大张旗鼓\n",
      "大雪纷飞\n",
      "天作之合\n",
      "奋发图强\n",
      "如释重负\n",
      "孜孜不倦\n",
      "学富五车\n",
      "学而不厌\n",
      "守口如瓶\n",
      "守株待兔\n",
      "安居乐业\n",
      "完璧归赵\n",
      "家喻户晓\n",
      "对牛弹琴\n",
      "山崩地裂\n",
      "山清水秀\n",
      "废寝忘食\n",
      "得不偿失\n",
      "心心相印\n",
      "心花怒放\n",
      "忠心耿耿\n",
      "患难与共\n",
      "悬梁刺股\n",
      "惊弓之鸟\n",
      "成竹在胸\n",
      "所向披靡\n",
      "才高八斗\n",
      "持之以恒\n",
      "指鹿为马\n",
      "排山倒海\n",
      "掩耳盗铃\n",
      "揭竿而起\n",
      "旗开得胜\n",
      "日出而作\n",
      "春暖花开\n",
      "望梅止渴\n",
      "欣欣向荣\n",
      "比翼双飞\n",
      "气势磅礴\n",
      "水滴石穿\n",
      "波澜壮阔\n",
      "洛阳纸贵\n",
      "浩浩荡荡\n",
      "海誓山盟\n",
      "温故知新\n",
      "滥竽充数\n",
      "滥竽充术\n",
      "班门弄斧\n",
      "琴瑟和鸣\n",
      "画蛇添足\n",
      "画龙点睛\n",
      "疾如雷霆\n",
      "白头偕老\n",
      "白雪皑皑\n",
      "目不暇接\n",
      "盲人摸象\n",
      "相濡以沫\n",
      "破釜沉舟\n",
      "硕果累累\n",
      "秋高气爽\n",
      "程门立雪\n",
      "笨鸟先飞\n",
      "精益求精\n",
      "繁花似锦\n",
      "纸上谈兵\n",
      "绿树成荫\n",
      "翻天覆地\n",
      "肝胆相照\n",
      "背水一战\n",
      "胸有成竹\n",
      "自相矛盾\n",
      "至死不渝\n",
      "舍己为人\n",
      "舍生取义\n",
      "草木皆兵\n",
      "草船借箭\n",
      "触类旁通\n",
      "言而有信\n",
      "负荆请罪\n",
      "赤壁鏖兵\n",
      "赤胆忠心\n",
      "走马观花\n",
      "运筹帷幄\n",
      "退避三舍\n",
      "釜底抽薪\n",
      "铁杵成针\n",
      "锐不可当\n",
      "门可罗雀\n",
      "闻鸡起舞\n",
      "雷厉风行\n",
      "雷霆万钧\n",
      "青出于蓝\n",
      "风和日丽\n",
      "风起云涌\n",
      "风驰电掣\n",
      "马到成功\n",
      "鸟语花香\n",
      "鸾凤和鸣\n",
      "鹤立鸡群\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def extract_distinct_words(csv_file, output_file):\n",
    "    \"\"\"\n",
    "    Read CSV file and extract all distinct words from W1, W2, W3 columns.\n",
    "    Save the distinct words to a text file.\n",
    "    \"\"\"\n",
    "    distinct_words = set()\n",
    "    \n",
    "    # Read the CSV file\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        # Skip the header row\n",
    "        next(file)\n",
    "        \n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if len(row) >= 3:  # Ensure row has at least 3 columns\n",
    "                # Extract W1, W2, W3 (columns 0, 1, 2)\n",
    "                w1 = row[0].strip()\n",
    "                w2 = row[1].strip()\n",
    "                w3 = row[2].strip()\n",
    "                \n",
    "                # Add non-empty words to the set\n",
    "                if w1:\n",
    "                    distinct_words.add(w1)\n",
    "                if w2:\n",
    "                    distinct_words.add(w2)\n",
    "                if w3:\n",
    "                    distinct_words.add(w3)\n",
    "    \n",
    "    # Sort the distinct words for better readability\n",
    "    sorted_words = sorted(distinct_words)\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for word in sorted_words:\n",
    "            file.write(word + '\\n')\n",
    "    \n",
    "    print(f\"Extracted {len(distinct_words)} distinct words.\")\n",
    "    print(f\"Words saved to: {output_file}\")\n",
    "    \n",
    "    return sorted_words\n",
    "\n",
    "\n",
    "# csv_filename = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp.csv\" \n",
    "# output_filename = \"/Users/yufang/WM_load/word_database_txt/three_3_syllable_words.txt\"\n",
    "\n",
    "csv_filename = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_syllable_trials.csv\" \n",
    "output_filename = \"/Users/yufang/WM_load/word_database_txt/three_4_syllable_words.txt\"\n",
    "\n",
    "csv_filename = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/four_3_syllable_trials.csv\"\n",
    "output_filename = \"/Users/yufang/WM_load/word_database_txt/four_3_syllable_words.txt\"\n",
    "\n",
    "try:\n",
    "    words = extract_distinct_words(csv_filename, output_filename)\n",
    "    print(\"\\nDistinct words:\")\n",
    "    for word in words:\n",
    "        print(word)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{csv_filename}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c098ba5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete! Output saved to /Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp_new.csv\n",
      "\n",
      "Processed 432 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_cue_position(row):\n",
    "    \"\"\"\n",
    "    Find the position of the cue character in W1.\n",
    "    Returns (word_position, char_position) where:\n",
    "    - word_position: 1 for W1, 2 for W2, 3 for W3\n",
    "    - char_position: 0-based index of character within the word\n",
    "    \"\"\"\n",
    "    cue = row['Cue']\n",
    "    \n",
    "    # Check W1 first\n",
    "    if cue in row['W1']:\n",
    "        return 1, row['W1'].index(cue)\n",
    "    # Check W2\n",
    "    elif cue in row['W2']:\n",
    "        return 2, row['W2'].index(cue)\n",
    "    # Check W3\n",
    "    elif cue in row['W3']:\n",
    "        return 3, row['W3'].index(cue)\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def transform_csv(input_file, output_file):\n",
    "    # Read the CSV, skipping empty columns\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Remove unnamed/empty columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # Keep only the necessary columns\n",
    "    df = df[['W1', 'W2', 'W3', 'Cue']]\n",
    "    \n",
    "    # Remove any rows with missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Extract cue position information\n",
    "    df[['Cue_Word', 'Cue_Pos']] = df.apply(\n",
    "        lambda row: pd.Series(extract_cue_position(row)), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to match desired output\n",
    "    df = df[['W1', 'W2', 'W3', 'Cue_Word', 'Cue_Pos', 'Cue']]\n",
    "    \n",
    "    # Save to new CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Transformation complete! Output saved to {output_file}\")\n",
    "    print(f\"\\nProcessed {len(df)} rows\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp.csv\"  # Change to your input file name\n",
    "    output_file = \"/Users/yufang/WM_load/Exp1_Syllable_Sequence_and_Cueing_Syllable_List/three_3_syllable_exp_new.csv\"  # Change to your desired output file name\n",
    "    \n",
    "    transform_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fd295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
